# -*- coding: utf-8 -*-
"""Kaggle(titanic_dataset).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CD3s6zTjv1Mw5ds5Kk5BSR-SRo1n6WNL

# **#Importing libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""# **#Importing dataset**"""

train = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')

train.shape

test.shape

"""# **#Preprocessing**"""

train.head()

test.head()

"""# **Preprocessing : [1] - Feature selection**"""

df_train = train.drop(columns=['PassengerId', 'Name' , 'Ticket'])

df_test = test.drop(columns=['PassengerId', 'Name' , 'Ticket'])

"""I dropped the Ticket because the Cabin and Ticket are very close features"""

df_train.head()

df_test.head()

"""# **Preprocessing : [2] - Handling missing values**

Handling the mising values for the training set
"""

df_train.isnull().sum()

"""TRAIN SET ::
Now i have 3 columns that contain missing values ......
Age , Cabin & Embarked
"""

fill_values = {'Age': df_train['Age'].mean(), 'Cabin': 'Unknown', 'Embarked': 'S'}
df_train.fillna(fill_values , inplace=True)

df_train.head()

"""Handling the missing values for the test set"""

df_test.isnull().sum()

"""TRAIN SET ::
Now i have 2 columns that contain missing values ......
Age & Cabin
"""

fill_values = {'Age': df_test['Age'].mean(), 'Cabin': 'Unknown', 'Fare': df_test['Fare'].mean()} # Added 'Fare' to fill_values
df_test.fillna(fill_values , inplace=True)

df_test.head()

"""# **Preprocessing : [3] - Handling outliers**"""

df_train['Age'].max()

df_train['Age'].min()

df_train['Age'].median()

"""After computing max , min and median of the Age column i concluded that the ages distribution is completely normal 0.42 <= Age <= 80"""

sns.histplot(df_train['Age'], kde=True)

plt.title('Age Distribution')
plt.show()

df_test['Age'].max()

df_test['Age'].min()

df_test['Age'].median()

sns.histplot(df_test['Age'], kde=True)

plt.title('Age Distribution')
plt.show()

"""Checking if the fares feature has an outlier"""

Q1 = df_train.Fare.quantile(0.25)
Q3 = df_train.Fare.quantile(0.75)

IQR = Q3 - Q1

IQR

df_train['Fare'].min()

df_train['Fare'].max()

sns.histplot(df_train['Fare'], kde=True)

plt.title('Age Distribution')
plt.show()

"""The data is left skewed which is fine because this has a relationship with the class column , The higher the class the higher the fare goes and as the higer class has significanlty less individuals therefore the data is left skewed or positively skewed."""

Q1 = df_test.Fare.quantile(0.25)
Q3 = df_test.Fare.quantile(0.75)

IQR = Q3 - Q1

IQR

df_test['Fare'].min()

df_test['Fare'].max()

sns.histplot(df_test['Fare'], kde=True)

plt.title('Age Distribution')
plt.show()

"""Same comment as the one for the training set

# **Preprocessing : [4] - Removing duplicates**
"""

df_train.duplicated().sum()

df_train.drop_duplicates(inplace=True)

df_train.duplicated().sum()

"""All the duplicated for the training set was removed"""

df_test.duplicated().sum()

# df_test.drop_duplicates(inplace=True) # Removed this line as it causes issues for Kaggle submission formatting

# df_test.duplicated().sum() # This check is no longer relevant if duplicates are not dropped

"""All the duplicates for the testing set was removed

# **Preprocessing:[5]-Splitting the training data into matrix of feature [X] and labels [y]**

Splitting the training set into X and y
"""

df_train.head()

X = df_train.iloc[: , 1:]
y = df_train.iloc[: , 0]

"""Placing the testing data into matrix of features X_test"""

df_test.head()

X_test = df_test.iloc[:,:]

"""# **Preprocessing:[6] - Encoding categorical data for both X_train and X_test**

I need to handle the categorical data and turn every data entry to a number before utilizing machine learning models
"""

X['Cabin'].unique()

# Create a new column 'Deck' by taking the first letter of 'Cabin'
X['Deck'] = X['Cabin'].str[0]

# Dropping the messy 'Cabin' column
X = X.drop('Cabin', axis=1)

# Check the new unique values
print(X['Deck'].unique())

X = pd.get_dummies(X, columns=['Deck'], prefix='Deck', dtype=int)

# Check the result
print(X.head())

X_test['Cabin'].unique()

# Create a new column 'Deck' by taking the first letter of 'Cabin'
X_test['Deck'] = X_test['Cabin'].str[0]

# Dropping the messy 'Cabin' column
X_test = X_test.drop('Cabin', axis=1)

# Check the new unique values
print(X_test['Deck'].unique())

X_test = pd.get_dummies(X_test, columns=['Deck'], prefix='Deck', dtype=int)

# Check the result
print(X_test.head())

X['Embarked'].unique()

X['Embarked'] = X['Embarked'].replace({'S': 0, 'C': 1, 'Q': 2})

X_test['Embarked'].unique()

X_test['Embarked'] = X_test['Embarked'].replace({'S': 0, 'C': 1, 'Q': 2})

X['Sex'].unique()

X['Sex'] = X['Sex'].replace({'male': 1, 'female': 0})

X_test['Sex'].unique()

X_test['Sex'] = X_test['Sex'].replace({'male': 1, 'female': 0})

"""# **Splitting the training data into training set and validation set**"""

from sklearn.model_selection import train_test_split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

"""# **Preprocessing:[7] - Apply Feature scaling**

As feature scaling is only applied for the training set i will only apply it for the matrix of features X_trained
"""

X.head(3)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X_train)

"""The upcoming cell is just to make sure that both columns in X_train and X_test are aligned in the columns"""

#Adding the missing column (Deck_T) to the test set and fill it with 0 as the training set has T column and the X_tested didn't
X_test['Deck_T'] = 0

#Get the master list of columns from X_trained
train_columns = X_train.columns

#Force X_tested to match that exact order : "Re-order X_tested to match X_trained's order"
X_tested_aligned = X_test[train_columns]

#To make sure there is no missing values
X_tested_aligned.fillna(X_tested_aligned['Fare'].mean() , inplace=True)

"""# **#Modeling**"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

"""# **#Prediction**"""

y_pred = classifier.predict(X_val)

"""# **#Evaluation**"""

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
print(confusion_matrix(y_val, y_pred))
print(accuracy_score(y_val, y_pred))
print(precision_score(y_val, y_pred))
print(recall_score(y_val, y_pred))
print(f1_score(y_val, y_pred))

"""# **Applying K fold cross validation**"""

from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = classifier, X = X_train , y = y_train , cv = 10)
print(accuracies.mean())
print(accuracies.std())

"""# **Final model**"""

from sklearn.linear_model import LogisticRegression

final_model = LogisticRegression()
# Train on the scaled training features (X) and corresponding training labels (y_train)
final_model.fit(X, y_train)

# Scale the aligned test set using the same scaler fitted on the training data
X_test_scaled = sc.transform(X_tested_aligned)

# Predict on the scaled test data
final_predictions = final_model.predict(X_test_scaled)

# Get the PassengerId from the original test data
passenger_ids = test['PassengerId']

# Create a DataFrame for the submission file
submission_df = pd.DataFrame({'PassengerId': passenger_ids, 'Survived': final_predictions})

# Display the first few rows of the submission DataFrame
display(submission_df.head())

# Save the submission DataFrame to a CSV file
submission_df.to_csv('submission.csv', index=False)

print("Submission file 'submission.csv' created successfully!")

"""You can now download the `submission.csv` file from your Colab environment and submit it to the Kaggle competition!"""

X.shape

y.shape